const fs = require('fs');
const csv = require('fast-csv');
const BillingLineItem = require('../models/BillingLineItem');
const normalize = require('./normalize');
const JobRun = require('../models/JobRun');
const config = require('../config');


async function ingestCsvStream(stream, filename, meta={}){
const job = await JobRun.create({ filename, startedAt: new Date(), rowsProcessed:0, status:'running' });
return new Promise((resolve, reject)=>{
const parser = csv.parse({ headers: true, ignoreEmpty: true })
.on('error', async error=>{
await job.updateOne({ status:'failed', error: error.message });
reject(error);
})
.on('data', async row=>{
parser.pause();
try{
const doc = normalize(row);
// idempotency: upsert by a unique key: billId + lineItemId if present, otherwise store raw and let duplicates be rare
const filter = { 'raw.lineItemId': doc.raw && doc.raw.lineItemId ? doc.raw.lineItemId : null, billDate: doc.billDate };
if(filter['raw.lineItemId']){
await BillingLineItem.updateOne(filter, { $set: doc }, { upsert: true });
} else {
await BillingLineItem.create(doc);
}
job.rowsProcessed++;
}catch(e){
// safe error handling: attach row to job's errors
job.errors = job.errors || [];
job.errors.push({ rowPreview: JSON.stringify(row).slice(0,200), error: e.message });
}
parser.resume();
})
.on('end', async rowCount=>{
job.endedAt = new Date();
job.status = 'completed';
await job.save();
resolve(job);
});


stream.pipe(parser);
});
}


module.exports = { ingestCsvStream };
